{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For reference, I have looked at their NeuroChem sublibrary of Torchani in their documentaion here https://aiqm.github.io/torchani/api.html#module-torchani.neurochem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy\n",
    "import torchani\n",
    "from torchani import ANIModel\n",
    "#from torchviz import make_dot\n",
    "from torchani import neurochem\n",
    "from torchani.utils import ChemicalSymbolsToInts as CSTI\n",
    "import os\n",
    "\n",
    "#os.environ[\"PATH\"] += os.pathsep + 'C:\\Program Files (x86)\\Graphviz-2.38\\bin'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I have found the information they have used to transfer their NeuroChem Networks to pyTorch. This is all stored in the 'ani-model-zoo-master' directory accessible here https://github.com/aiqm/ani-model-zoo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Model compromises of 3 parts: AEVcomputer, ANI-Network, and the EnergyShifter(to account for atom's self energy at the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#We want to use the ani-2x_8x model so ive set the path to there\n",
    "model_path = os.path.join(local,r\"ani-model-zoo-master\\resources\\ani-2x_8x\")\n",
    "\n",
    "#Extracting constants used for their AEV computer\n",
    "const = neurochem.Constants(os.path.join(model_path, 'rHCNOSFCl-5.1R_16-3.5A_a8-4.params'))\n",
    "aev_computer = torchani.AEVComputer(Rcr=const.Rcr, Rca=const.Rca, EtaR=const.EtaR, ShfR=const.ShfR, \n",
    "                                    EtaA=const.EtaA, Zeta=const.Zeta, ShfA=const.ShfA, ShfZ=const.ShfZ,\n",
    "                                   num_species = const.num_species)\n",
    "\n",
    "\n",
    "\n",
    "#Exracting the self-atomic energies for the energy shifter\n",
    "EShifter = neurochem.load_sae(os.path.join(model_path, 'sae_linfit.dat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can either load a single atomic network or a full model (or an ensemble of models too but I am not sure that will work with transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I do not know what number next to train does in each folder, I've assumed train7 would be the best\n",
    "\n",
    "\n",
    "#Here we load an H network\n",
    "H_Network = neurochem.load_atomic_network(os.path.join(model_path, r'train7\\networks\\ANN-H.nnf'))\n",
    "\n",
    "#Loading a full model incorprating atoms F O C for example we would instead\n",
    "FOC_Network = neurochem.load_model(['F', 'O','C'], os.path.join(model_path, r'train7\\networks'))\n",
    "\n",
    "#In our case ofcourse we would like to to load the ani2 model with al the atoms\n",
    "ANI2_Network = neurochem.load_model(const.species, os.path.join(model_path,r'train7\\networks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 1, 3]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing things on a simple example CO2\n",
    "species_to_tensor = CSTI(['H','O',\"F\", 'C'])\n",
    "\n",
    "coordinates = torch.tensor([[[-1.0,0.0,0.0],\n",
    "                             [0.0,0.0,0.0],\n",
    "                             [1.0,0.0,0.0]]])\n",
    "species = species_to_tensor(['O', 'C', 'O']).unsqueeze(0)\n",
    "\n",
    "species2 = const.species_to_tensor(['O','C','O']).unsqueeze(0)\n",
    "\n",
    "\n",
    "aev = aev_computer((species,coordinates))\n",
    "\n",
    "#This network does the sum\n",
    "y = FOC_Network(aev)\n",
    "\n",
    "y = EShifter(y)\n",
    "\n",
    "species2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the AEV through the H network didn't seem to work and looked like a pain to fix. Instead I overrided the ANIModel (in this case FOC- Model) to return the individual energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeciesEnergies(species=tensor([[1, 3, 1]]), energies=tensor([[-151.2266, -151.3697, -151.2266]], dtype=torch.float64,\n",
       "       grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing with H2 Molecule\n",
    "\n",
    "coordinates1 = torch.tensor([[[0.75,0,0],\n",
    "                            [0,0,0]]])\n",
    "species1 = species_to_tensor(['H']).unsqueeze(0)\n",
    "\n",
    "aev1 = aev_computer((species1, coordinates1))\n",
    "\n",
    "#z = H_Network(aev1)\n",
    "#z = EShifter(z)\n",
    "aev1\n",
    "\n",
    "#Overriding the FOC-Model\n",
    "from typing import NamedTuple, Tuple\n",
    "\n",
    "class SpeciesEnergies(NamedTuple):\n",
    "    species: Tensor\n",
    "    energies: Tensor\n",
    "\n",
    "\n",
    "\n",
    "def overforward(self, species_aev: Tuple[Tensor, Tensor]) -> SpeciesEnergies:\n",
    "    species, aev = species_aev\n",
    "    species_ = species.flatten()\n",
    "    aev = aev.flatten(0, 1)\n",
    "\n",
    "    output = aev.new_zeros(species_.shape)\n",
    "\n",
    "    for i, (_, m) in enumerate(self.items()):\n",
    "        mask = (species_ == i)\n",
    "        midx = mask.nonzero().flatten()\n",
    "        if midx.shape[0] > 0:\n",
    "            input_ = aev.index_select(0, midx)\n",
    "            output.masked_scatter_(mask, m(input_).flatten())\n",
    "    output = output.view_as(species)\n",
    "    return SpeciesEnergies(species, output)\n",
    "\n",
    "\n",
    "funcType = type(FOC_Network.forward)\n",
    "\n",
    "FOC_Network.forward = funcType(overforward, FOC_Network)\n",
    "\n",
    "\n",
    "FOC_Network(aev)\n",
    "EShifter(FOC_Network(aev))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have the individual energies :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the single torchani_update.py script I made for the code to do this for ani2 :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
